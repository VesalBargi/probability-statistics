{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcde5614",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e15bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "from math import lgamma\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02c70d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Two Currencies CSV\n",
    "df = pd.read_csv(\"two_currencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Student Performance CSV\n",
    "df = pd.read_csv(\"student_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb80b1",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "---\n",
    "\n",
    "**Expected Portfolio Profit**\n",
    "\n",
    "$\n",
    "E[\\Pi] = b_A E[P_A] + b_B E[P_B]\n",
    "$\n",
    "\n",
    "**Constraint**\n",
    "\n",
    "$\n",
    "b_A + b_B = 20{,}000\n",
    "$\n",
    "\n",
    "**Portfolio Variance**\n",
    "\n",
    "$\n",
    "\\operatorname{Var}(\\Pi) = b_A^2 \\, \\operatorname{Var}(P_A) + b_B^2 \\, \\operatorname{Var}(P_B) + 2 b_A b_B \\, \\operatorname{Cov}(P_A, P_B)\n",
    "$\n",
    "\n",
    "**Optimal Allocation (Minimum Variance)**\n",
    "\n",
    "$\n",
    "b_A^* = 20{,}000 \\cdot \\frac{\\operatorname{Var}(P_B) - \\operatorname{Cov}(P_A, P_B)}{\\operatorname{Var}(P_A) + \\operatorname{Var}(P_B) - 2 \\operatorname{Cov}(P_A, P_B)}\n",
    "$\n",
    "\n",
    "$\n",
    "b_B^* = 20{,}000 - b_A^*\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract profits (these are for $10,000 investment)\n",
    "PA = df[\"profit_A\"].values\n",
    "PB = df[\"profit_B\"].values\n",
    "\n",
    "# Part (a)\n",
    "# Maximize expected profit\n",
    "mean_PA = np.mean(PA)\n",
    "mean_PB = np.mean(PB)\n",
    "\n",
    "print(\"Mean profit A (per $10k):\", mean_PA)\n",
    "print(\"Mean profit B (per $10k):\", mean_PB)\n",
    "\n",
    "if mean_PA > mean_PB:\n",
    "    print(\"Max profit: invest all in A\")\n",
    "elif mean_PA < mean_PB:\n",
    "    print(\"Max profit: invest all in B\")\n",
    "else:\n",
    "    print(\"Any split works, means are equal.\")\n",
    "\n",
    "# Part (b)\n",
    "# Minimize variance (risk)\n",
    "var_A = np.var(PA, ddof=1)\n",
    "var_B = np.var(PB, ddof=1)\n",
    "cov_AB = np.cov(PA, PB, ddof=1)[0, 1]\n",
    "\n",
    "bA_opt = 20000 * (var_B - cov_AB) / (var_A + var_B - 2 * cov_AB)\n",
    "bB_opt = 20000 - bA_opt\n",
    "\n",
    "print(\"Optimal b_A for minimum variance:\", bA_opt)\n",
    "print(\"Optimal b_B for minimum variance:\", bB_opt)\n",
    "\n",
    "# Grid search to verify\n",
    "amounts = np.linspace(0, 20000, 201)\n",
    "variances = []\n",
    "\n",
    "for bA in amounts:\n",
    "    bB = 20000 - bA\n",
    "    profits_scaled_A = (bA / 10000) * PA\n",
    "    profits_scaled_B = (bB / 10000) * PB\n",
    "    portfolio_profits = profits_scaled_A + profits_scaled_B\n",
    "    variances.append(np.var(portfolio_profits, ddof=1))\n",
    "\n",
    "min_idx = np.argmin(variances)\n",
    "print(\n",
    "    \"Grid search min variance allocation:\", amounts[min_idx], 20000 - amounts[min_idx]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa128d",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "---\n",
    "\n",
    "**Hypotheses**\n",
    "\n",
    "$\n",
    "H_0 : \\mu_{\\text{female}} = \\mu_{\\text{male}}\n",
    "$\n",
    "\n",
    "$\n",
    "H_A : \\mu_{\\text{female}} \\neq \\mu_{\\text{male}}\n",
    "$\n",
    "\n",
    "**Correlation (Pearson)**\n",
    "\n",
    "$\n",
    "r_{XY} = \\frac{\\operatorname{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\n",
    "$\n",
    "\n",
    "**Sample covariance**\n",
    "\n",
    "$\n",
    "\\operatorname{Cov}(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "$\n",
    "\n",
    "**Sample standard deviation**\n",
    "\n",
    "$\n",
    "\\sigma_X = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$\n",
    "\n",
    "\n",
    "**Two-sample Welch’s t-test statistic**\n",
    "\n",
    "$\n",
    "t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "$\n",
    "\n",
    "**Degrees of freedom (Welch–Satterthwaite equation)**\n",
    "\n",
    "$\n",
    "df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(\\tfrac{s_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\tfrac{s_2^2}{n_2}\\right)^2}{n_2 - 1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sex: female=1, male=0\n",
    "df[\"sex_code\"] = df[\"sex\"].map({\"F\": 1, \"M\": 0})\n",
    "\n",
    "# Part (a)\n",
    "# Compute correlation\n",
    "r = df[\"sex_code\"].corr(df[\"G1\"])\n",
    "print(\"Correlation between gender and G1:\", r)\n",
    "\n",
    "# Part (b)\n",
    "# Split into two groups\n",
    "females = df[df[\"sex\"] == \"F\"][\"G1\"]\n",
    "males = df[df[\"sex\"] == \"M\"][\"G1\"]\n",
    "\n",
    "# Welch’s df\n",
    "n1 = len(females)\n",
    "n2 = len(males)\n",
    "s1 = np.var(females, ddof=1)\n",
    "s2 = np.var(males, ddof=1)\n",
    "\n",
    "dof = ((s1 / n1 + s2 / n2) ** 2) / (\n",
    "    ((s1 / n1) ** 2) / (n1 - 1) + ((s2 / n2) ** 2) / (n2 - 1)\n",
    ")\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "\n",
    "# Welch’s t-test\n",
    "t_stat, p_val = stats.ttest_ind(females, males, equal_var=False)\n",
    "\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_val)\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject H0: Significant difference in average G1.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: No significant difference in average G1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01094866",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "---\n",
    "\n",
    "**Hypotheses**\n",
    "\n",
    "$\n",
    "H_0 : \\mu_{G1} = \\mu_{G2}\n",
    "$\n",
    "\n",
    "$\n",
    "H_A : \\mu_{G1} \\neq \\mu_{G2}\n",
    "$\n",
    "\n",
    "**Paired t-test statistic**\n",
    "\n",
    "$\n",
    "t = \\frac{\\bar{d}}{s_d / \\sqrt{n}}\n",
    "$\n",
    "\n",
    "**Where**\n",
    "\n",
    "$\n",
    "d_i = G1_i - G2_i\n",
    "$\n",
    "\n",
    "$\n",
    "\\bar{d} = \\frac{1}{n} \\sum_{i=1}^n d_i\n",
    "$\n",
    "\n",
    "$\n",
    "s_d^2 = \\frac{1}{n-1} \\sum_{i=1}^n (d_i - \\bar{d})^2\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between grades\n",
    "diff = df[\"G1\"] - df[\"G2\"]\n",
    "# Paired t-test on G1 and G2\n",
    "t_stat, p_val = stats.ttest_rel(df[\"G1\"], df[\"G2\"])\n",
    "\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_val)\n",
    "\n",
    "# One-sample t-test on difference vs zero\n",
    "t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_val)\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject H0: Significant difference between G1 and G2.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: No significant difference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b46a21",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "---\n",
    "\n",
    "**Parameter and estimator**\n",
    "\n",
    "$\n",
    "\\mu = E[G3], \\quad \\hat{\\mu} = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\n",
    "$\n",
    "\n",
    "**Empirical distribution (nonparametric bootstrap)**\n",
    "\n",
    "$\n",
    "\\hat{F}_n = \\frac{1}{n}\\sum_{i=1}^n \\delta_{X_i}\n",
    "$\n",
    "\n",
    "**Bootstrap resamples and replicates (nonparametric)**\n",
    "\n",
    "$\n",
    "X^{*b}_1,\\dots,X^{*b}_n \\overset{i.i.d.}{\\sim} \\hat{F}_n,\\quad \n",
    "\\hat{\\mu}^{*b} = \\frac{1}{n}\\sum_{i=1}^n X^{*b}_i,\\quad b=1,\\dots,B\n",
    "$\n",
    "\n",
    "**Percentile confidence interval (level $0.9$)**\n",
    "\n",
    "$\n",
    "\\text{CI}_{0.9}^{\\text{perc}} = \\left[\\, Q_{0.05}\\big(\\{\\hat{\\mu}^{*b}\\}_{b=1}^B\\big),\\; Q_{0.95}\\big(\\{\\hat{\\mu}^{*b}\\}_{b=1}^B\\big) \\,\\right]\n",
    "$\n",
    "\n",
    "**Bootstrap standard error (optional)**\n",
    "\n",
    "$\n",
    "\\widehat{SE}_{\\text{boot}}(\\hat{\\mu}) = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B \\left(\\hat{\\mu}^{*b}-\\overline{\\hat{\\mu}^{*}}\\right)^2},\\quad \n",
    "\\overline{\\hat{\\mu}^{*}}=\\frac{1}{B}\\sum_{b=1}^B \\hat{\\mu}^{*b}\n",
    "$\n",
    "\n",
    "**Parametric bootstrap (example: normal fit)**\n",
    "\n",
    "$\n",
    "\\hat{\\mu}=\\bar{X},\\quad \\hat{\\sigma}^2 = s^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2,\n",
    "$\n",
    "\n",
    "$\n",
    "X^{*b}_i \\overset{i.i.d.}{\\sim} \\mathcal{N}(\\hat{\\mu},\\hat{\\sigma}^2),\\quad \n",
    "\\hat{\\mu}^{*b}=\\frac{1}{n}\\sum_{i=1}^n X^{*b}_i,\\quad \n",
    "\\text{CI}_{0.9}^{\\text{perc}}=\\left[Q_{0.05},\\,Q_{0.95}\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bad42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = df[\"G3\"].values\n",
    "B = 10000\n",
    "boot_means = []\n",
    "\n",
    "# Generate bootstrap samples and compute means\n",
    "np.random.seed(0)\n",
    "for _ in range(B):\n",
    "    sample = np.random.choice(g3, size=len(g3), replace=True)\n",
    "    boot_means.append(sample.mean())\n",
    "\n",
    "# Calculate 90% confidence interval using percentiles\n",
    "lower = np.percentile(boot_means, 5)\n",
    "upper = np.percentile(boot_means, 95)\n",
    "\n",
    "print(f\"0.9 Bootstrap CI for mean G3: ({lower:.2f}, {upper:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7dd563",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "---\n",
    "\n",
    "**Sample mean**\n",
    "\n",
    "$\n",
    "\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\n",
    "$\n",
    "\n",
    "**Sample standard deviation**\n",
    "\n",
    "$\n",
    "s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2}\n",
    "$\n",
    "\n",
    "**Central Limit Theorem (approximate sampling distribution)**\n",
    "\n",
    "$\n",
    "\\bar{X} \\;\\approx\\; \\mathcal{N}\\!\\left(\\mu,\\; \\frac{\\sigma^2}{n}\\right)\n",
    "$\n",
    "\n",
    "**Standard error (estimated)**\n",
    "\n",
    "$\n",
    "SE = \\frac{s}{\\sqrt{n}}\n",
    "$\n",
    "\n",
    "**Critical value for 90% CI (two-sided)**\n",
    "\n",
    "$\n",
    "z_{0.95} = 1.645\n",
    "$\n",
    "\n",
    "**Margin of error**\n",
    "\n",
    "$\n",
    "ME = z_{0.95} \\cdot SE\n",
    "$\n",
    "\n",
    "**Confidence interval (CLT-based)**\n",
    "\n",
    "$\n",
    "\\text{CI}_{0.9} = \\Big[\\, \\bar{X} - ME,\\; \\bar{X} + ME \\,\\Big]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_g3 = np.mean(g3)\n",
    "std_g3 = np.std(g3, ddof=1)\n",
    "n = len(g3)\n",
    "z_crit = 1.645\n",
    "\n",
    "# Margin of error\n",
    "margin = z_crit * (std_g3 / math.sqrt(n))\n",
    "\n",
    "# Lower and upper bounds of 90% confidence interval\n",
    "lower_clt = mean_g3 - margin\n",
    "upper_clt = mean_g3 + margin\n",
    "\n",
    "print(f\"0.9 CLT CI for mean G3: ({lower_clt:.2f}, {upper_clt:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a442a39",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "---\n",
    "\n",
    "### Compare bootstrap vs CLT CIs\n",
    "\n",
    "* Both methods provide very similar 0.9 confidence intervals:  \n",
    "  * Bootstrap CI: (11.69, 12.11)  \n",
    "  * CLT CI: (11.70, 12.11)  \n",
    "* This similarity indicates that the sample size is large enough for the CLT to hold, making its normality assumption appropriate.  \n",
    "* Bootstrap does not rely on parametric assumptions and uses resampling to estimate the CI, making it more flexible.  \n",
    "* The slight difference in lower bound is negligible and due to bootstrap capturing sample variability directly.  \n",
    "* Overall, the consistent results show both methods are reliable for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204b285",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "---\n",
    "\n",
    "**Likelihood (Poisson, iid)**\n",
    "\n",
    "$\n",
    "L(\\lambda; x_1,\\dots,x_n) \\;=\\; \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!}\n",
    "$\n",
    "\n",
    "**Log-likelihood**\n",
    "\n",
    "$\n",
    "\\ell(\\lambda)=\\log L(\\lambda)= -n\\lambda + \\left(\\sum_{i=1}^n x_i\\right)\\log\\lambda - \\sum_{i=1}^n \\log(x_i!)\n",
    "$\n",
    "\n",
    "**Maximum Likelihood Estimator (MLE)**\n",
    "\n",
    "$\n",
    "\\hat{\\lambda} = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i\n",
    "$\n",
    "\n",
    "**Log-likelihood at $\\hat\\lambda$**\n",
    "\n",
    "$\n",
    "\\ell(\\hat{\\lambda}) = -n\\hat{\\lambda} + \\left(\\sum_{i=1}^n x_i\\right)\\log\\hat{\\lambda} - \\sum_{i=1}^n \\log(x_i!)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be354828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract absences column, drop missing\n",
    "absences = df[\"absences\"]\n",
    "\n",
    "# Summary stats\n",
    "n = len(absences)\n",
    "sum_x = absences.sum()\n",
    "x_bar = absences.mean()\n",
    "\n",
    "print(\"Number of students (n):\", n)\n",
    "print(\"Total absences (Σ xi):\", sum_x)\n",
    "print(\"Sample mean (x̄):\", x_bar)\n",
    "\n",
    "# Poisson MLE (λ̂ = sample mean)\n",
    "lambda_hat = x_bar\n",
    "print(\"Poisson MLE λ̂ =\", lambda_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a7b1a",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "---\n",
    "\n",
    "**Exponential PDF:**\n",
    "\n",
    "$\n",
    "f(t;\\lambda) = \\lambda e^{-\\lambda t}, \\quad t \\ge 0\n",
    "$\n",
    "\n",
    "**Interval probability (discretization):**\n",
    "\n",
    "$\n",
    "P(X = x) \\approx \\int_{x-0.5}^{x+0.5} \\lambda e^{-\\lambda t} \\, dt\n",
    "= e^{-\\lambda (x-0.5)} \\big(1 - e^{-\\lambda}\\big)\n",
    "$\n",
    "\n",
    "**Log-likelihood:**\n",
    "\n",
    "$\n",
    "\\ell(\\lambda) = \\sum_{i=1}^n \\log P(X=x_i) \n",
    "= -\\lambda \\sum_{i=1}^n (x_i - 0.5) + n \\log\\big(1 - e^{-\\lambda}\\big)\n",
    "$\n",
    "\n",
    "**Score equation:**\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial \\lambda} \\ell(\\lambda) = -S + \\frac{n e^{-\\lambda}}{1 - e^{-\\lambda}} = 0\n",
    "$\n",
    "\n",
    "**MLE solution:**\n",
    "\n",
    "$\n",
    "\\hat{\\lambda} = \\log\\!\\left(1 + \\frac{n}{S}\\right), \\quad S = \\sum_{i=1}^n (x_i - 0.5)\n",
    "$\n",
    "\n",
    "**Log-likelihood at MLE:**\n",
    "\n",
    "$\n",
    "\\ell(\\hat{\\lambda}) = -\\hat{\\lambda} S + n \\log\\big(1 - e^{-\\hat{\\lambda}}\\big)\n",
    "$\n",
    "\n",
    "A Poisson model gives probabilities for exact integer counts, while an exponential model gives continuous densities. Since densities aren't directly comparable to probabilities, we convert the exponential density into interval probabilities around each integer (e.g., [x−0.5, x+0.5)) to fairly compare it with the Poisson probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90feecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential MLE\n",
    "S = sum_x - 0.5 * n\n",
    "lambda_hat_exp = math.log(1.0 + n / S)\n",
    "\n",
    "loglik_exp = -lambda_hat_exp * S + n * math.log(1 - math.exp(-lambda_hat_exp))\n",
    "\n",
    "print(\"Exponential model (interval method):\")\n",
    "print(\"  λ̂ =\", lambda_hat_exp)\n",
    "print(\"  log-likelihood =\", loglik_exp)\n",
    "\n",
    "# Log-likelihood at λ̂ (Question 9)\n",
    "log_factorials = sum(lgamma(int(x) + 1) for x in absences)\n",
    "loglik = -n * lambda_hat + sum_x * math.log(lambda_hat) - log_factorials\n",
    "print(\"Poisson Log-likelihood at λ̂:\", loglik)\n",
    "\n",
    "# Compare Poisson vs Exponential\n",
    "if loglik > loglik_exp:\n",
    "    print(\"Poisson provides the better fit (higher likelihood).\")\n",
    "elif loglik < loglik_exp:\n",
    "    print(\"Exponential provides the better fit (higher likelihood).\")\n",
    "else:\n",
    "    print(\"Both models fit equally well (same likelihood).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e8eac",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "---\n",
    "\n",
    "**Empirical probability mass function (empirical PDF):**\n",
    "\n",
    "$\n",
    "\\hat{p}(x) = \\frac{\\#\\{i : X_i = x\\}}{n}, \\quad x \\in \\mathbb{Z}_{\\ge 0}\n",
    "$\n",
    "\n",
    "**Poisson PMF with MLE $\\hat{\\lambda}$:**\n",
    "\n",
    "$\n",
    "p(x; \\hat{\\lambda}) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^x}{x!}, \\quad x \\in \\mathbb{Z}_{\\ge 0}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba64467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical PDF\n",
    "values, counts = np.unique(absences, return_counts=True)\n",
    "empirical_pdf = counts / n\n",
    "\n",
    "# Poisson pmf using λ̂\n",
    "x_vals = np.arange(0, max(absences) + 1)\n",
    "poisson_pmf = poisson.pmf(x_vals, mu=lambda_hat)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(values, empirical_pdf, width=0.6, alpha=0.6, label=\"Empirical PDF\")\n",
    "plt.plot(x_vals, poisson_pmf, \"ro-\", label=f\"Poisson(λ={lambda_hat:.2f}) PMF\")\n",
    "plt.xlabel(\"Absences\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Empirical PDF vs Poisson likelihood function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7799b17",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "---\n",
    "\n",
    "**Empirical CDF:**\n",
    "\n",
    "$\n",
    "\\hat{F}(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\leq x\\}, \\quad x \\in \\mathbb{R}\n",
    "$\n",
    "\n",
    "**Poisson CDF with MLE $\\hat{\\lambda}$:**\n",
    "\n",
    "$\n",
    "F(x; \\hat{\\lambda}) = \\Pr(X \\leq x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^k}{k!}, \\quad x \\in \\mathbb{Z}_{\\ge 0}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical CDF\n",
    "sorted_abs = np.sort(absences)\n",
    "empirical_cdf = np.arange(1, n + 1) / n\n",
    "\n",
    "# Poisson CDF\n",
    "poisson_cdf = poisson.cdf(x_vals, mu=lambda_hat)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.step(sorted_abs, empirical_cdf, where=\"post\", label=\"Empirical CDF\")\n",
    "plt.plot(x_vals, poisson_cdf, \"r-\", label=f\"Poisson(λ={lambda_hat:.2f}) CDF\")\n",
    "plt.xlabel(\"Absences\")\n",
    "plt.ylabel(\"Cumulative probability\")\n",
    "plt.title(\"Empirical CDF vs Poisson CDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d576c3",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "---\n",
    "\n",
    "### Does the Distribution Fit?\n",
    "\n",
    "* **PDF (Q11):**\n",
    "\n",
    "  * Empirical PDF has a large spike at **0 absences**, much higher than Poisson predicts.\n",
    "  * Poisson fits moderately well for values around **2–6 absences**.\n",
    "  * Poisson decays too fast → underestimates the **heavy tail** (students with many absences).\n",
    "\n",
    "* **CDF (Q12):**\n",
    "\n",
    "  * Poisson CDF rises faster than empirical → puts too much probability on **small values**.\n",
    "  * Empirical CDF is more gradual, reflecting a **wider spread** in the data.\n",
    "\n",
    "**Conclusion**: The Poisson distribution does not fully capture the absences data. It misses the excess zeros and longer tail, suggesting overdispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe1984",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "\n",
    "---\n",
    "\n",
    "**Hypotheses**\n",
    "\n",
    "* $H_0 :$ Fjob and famsize are independent.\n",
    "* $H_A :$ Fjob and famsize are dependent.\n",
    "\n",
    "**Expected counts under independence**\n",
    "\n",
    "$\n",
    "E_{ij} = \\frac{(\\text{row total}_i)(\\text{column total}_j)}{n}\n",
    "$\n",
    "\n",
    "**Chi-square statistic**\n",
    "\n",
    "$\n",
    "\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n",
    "$\n",
    "\n",
    "**Degrees of freedom**\n",
    "\n",
    "$\n",
    "df = (r - 1)(c - 1)\n",
    "$\n",
    "\n",
    "**p-value (Chi-square distribution)**\n",
    "\n",
    "$\n",
    "p = 1 - F_{\\chi^2}(\\chi^2; df)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5794ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build observed contingency table\n",
    "ct = pd.crosstab(df[\"Fjob\"], df[\"famsize\"])\n",
    "print(\"Observed contingency table:\")\n",
    "print(ct)\n",
    "\n",
    "# Manual calculation (explicit)\n",
    "observed = ct.values.astype(float)\n",
    "row_totals = observed.sum(axis=1, keepdims=True)\n",
    "col_totals = observed.sum(axis=0, keepdims=True)\n",
    "n = observed.sum()\n",
    "expected = (row_totals @ col_totals) / n\n",
    "\n",
    "# check small expected counts\n",
    "print(\"\\nExpected counts (under independence):\")\n",
    "print(pd.DataFrame(expected, index=ct.index, columns=ct.columns))\n",
    "\n",
    "# compute Pearson's X^2\n",
    "chi2_stat = ((observed - expected) ** 2 / expected).sum()\n",
    "r, c = observed.shape\n",
    "dof = (r - 1) * (c - 1)\n",
    "p_val = 1 - stats.chi2.cdf(chi2_stat, dof)\n",
    "\n",
    "print(\"\\nManual chi-square statistic:\", chi2_stat)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(\"p-value:\", p_val)\n",
    "\n",
    "# Using scipy convenience function (returns same statistic)\n",
    "chi2_stat, p_val, dof, expected = stats.contingency.chi2_contingency(observed, correction=False)\n",
    "\n",
    "print(\"\\nscipy chi-square statistic:\", chi2_stat)\n",
    "print(\"scipy degrees of freedom:\", dof)\n",
    "print(\"scipy p-value:\", p_val)\n",
    "\n",
    "# expected returned by scipy should match expected\n",
    "print(\"\\nExpected counts (from scipy):\")\n",
    "print(pd.DataFrame(expected, index=ct.index, columns=ct.columns))\n",
    "\n",
    "# Decision at alpha = 0.05\n",
    "alpha = 0.05\n",
    "if p_val < alpha:\n",
    "    print(\"\\nReject H0: Evidence of dependence between Fjob and famsize.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject H0: No evidence of dependence between Fjob and famsize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470c322",
   "metadata": {},
   "source": [
    "## Question 15\n",
    "\n",
    "---\n",
    "\n",
    "**Sample correlation (Pearson’s r)**\n",
    "\n",
    "$\n",
    "r_{XY} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1) s_X s_Y}\n",
    "$\n",
    "\n",
    "**Sample covariance (inside numerator)**\n",
    "\n",
    "$\n",
    "\\text{Cov}(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "$\n",
    "\n",
    "**Standard deviations**\n",
    "\n",
    "$\n",
    "s_X = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad \n",
    "s_Y = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"famrel\", \"freetime\", \"goout\", \"Dalc\", \"Walc\", \"health\", \"G3\"]\n",
    "\n",
    "# Compute correlations with G3\n",
    "corrs = df[cols].corr()[\"G3\"].drop(\"G3\")  # correlations with G3 only\n",
    "print(\"Correlations with G3:\")\n",
    "print(corrs)\n",
    "\n",
    "# Find strongest\n",
    "strongest_var = corrs.abs().idxmax()\n",
    "print(f\"\\nStrongest correlation: {strongest_var}, r = {corrs[strongest_var]:.3f}\")\n",
    "\n",
    "# Visualization 1: correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[cols].corr(), annot=True, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation matrix (G3 vs predictors)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c146ee3",
   "metadata": {},
   "source": [
    "## Question 16\n",
    "\n",
    "---\n",
    "\n",
    "**Hypotheses**\n",
    "\n",
    "$\n",
    "H_0 : \\rho \\leq -0.2\n",
    "$\n",
    "\n",
    "$\n",
    "H_A : \\rho > -0.2\n",
    "$\n",
    "\n",
    "**Bootstrap resampling:**\n",
    "\n",
    "For $b = 1, 2, \\dots, B$:\n",
    "\n",
    "$\n",
    "r^{*(b)} = \\text{corr}(X^*_b, Y^*_b)\n",
    "$\n",
    "\n",
    "where $X^*_b, Y^*_b$ are sampled with replacement from the original data.\n",
    "\n",
    "**Bootstrap p-value:**\n",
    "\n",
    "$\n",
    "p = \\frac{1}{B} \\sum_{b=1}^B I\\big(r^{*(b)} \\leq -0.2\\big)\n",
    "$\n",
    "\n",
    "where $I(\\cdot)$ is an indicator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb93977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract variables\n",
    "x = df[\"Walc\"].values\n",
    "y = df[\"G3\"].values\n",
    "n = len(x)\n",
    "\n",
    "# Observed correlation\n",
    "obs_corr = np.corrcoef(x, y)[0, 1]\n",
    "print(\"Observed correlation:\", obs_corr)\n",
    "\n",
    "# Bootstrap\n",
    "B = 10000\n",
    "boot_corrs = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for _ in range(B):\n",
    "    # resample indices with replacement\n",
    "    idx = np.random.choice(n, size=n, replace=True)\n",
    "    x_resample = x[idx]\n",
    "    y_resample = y[idx]\n",
    "    boot_corrs.append(np.corrcoef(x_resample, y_resample)[0, 1])\n",
    "\n",
    "boot_corrs = np.array(boot_corrs)\n",
    "\n",
    "# p-value: proportion of bootstrap correlations ≤ -0.2\n",
    "p_value = np.mean(boot_corrs <= -0.2)\n",
    "\n",
    "print(f\"Bootstrap p-value for H0: ρ <= -0.2 vs HA: ρ > -0.2 = {p_value:.4f}\")\n",
    "\n",
    "# Decision at alpha = 0.05\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject H0: evidence that correlation is greater than -0.2\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: not enough evidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad593e78",
   "metadata": {},
   "source": [
    "## Question 18\n",
    "\n",
    "---\n",
    "\n",
    "**Slope (β₁):**\n",
    "\n",
    "$\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$\n",
    "\n",
    "**Intercept (β₀):**\n",
    "\n",
    "$\n",
    "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n",
    "$\n",
    "\n",
    "**Regression line:**\n",
    "\n",
    "$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"Dalc\"].values\n",
    "y = df[\"G3\"].values\n",
    "\n",
    "# Means\n",
    "x_bar = np.mean(x)\n",
    "y_bar = np.mean(y)\n",
    "\n",
    "# Slope (beta1)\n",
    "beta1 = np.sum((x - x_bar) * (y - y_bar)) / np.sum((x - x_bar) ** 2)\n",
    "\n",
    "# Intercept (beta0)\n",
    "beta0 = y_bar - beta1 * x_bar\n",
    "\n",
    "print(f\"Regression equation: G3 = {beta0:.3f} + {beta1:.3f} * Dalc\")\n",
    "print(f\"Intercept: {beta0:.3f} (expected grade when Dalc=0)\")\n",
    "print(f\"Slope: {beta1:.3f} (grade change per unit increase in Dalc)\")\n",
    "\n",
    "# Predictions for plotting\n",
    "y_hat = beta0 + beta1 * x\n",
    "\n",
    "# Scatterplot + regression line\n",
    "plt.scatter(x, y, alpha=0.5, label=\"Data\")\n",
    "plt.plot(x, y_hat, color=\"red\", label=\"Fitted line\")\n",
    "plt.xlabel(\"Dalc (workday alcohol consumption)\")\n",
    "plt.ylabel(\"G3 (final grade)\")\n",
    "plt.title(\"Simple Linear Regression: G3 vs Dalc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698ae63",
   "metadata": {},
   "source": [
    "## Question 19\n",
    "\n",
    "---\n",
    "\n",
    "**Likelihood (Bernoulli trials):**\n",
    "\n",
    "$\n",
    "L(\\theta) = \\theta^k (1 - \\theta)^{n-k}\n",
    "$\n",
    "\n",
    "**Bayes numerator (unnormalized posterior):**\n",
    "\n",
    "$\n",
    "\\text{num}(\\theta) = \\pi(\\theta) \\cdot L(\\theta)\n",
    "$\n",
    "\n",
    "**Posterior (normalized):**\n",
    "\n",
    "$\n",
    "p(\\theta \\mid \\text{data}) = \\frac{\\pi(\\theta) \\cdot L(\\theta)}{\\int_0^1 \\pi(\\theta) \\cdot L(\\theta) \\, d\\theta}\n",
    "$\n",
    "\n",
    "**Log-likelihood (numerical stability):**\n",
    "\n",
    "$\n",
    "\\log L(\\theta) = k \\log(\\theta) + (n-k)\\log(1-\\theta)\n",
    "$\n",
    "\n",
    "(Then exponentiate after centering by `max`.)\n",
    "\n",
    "**Theoretical posterior (Beta conjugacy):**\n",
    "\n",
    "$\n",
    "\\theta \\mid \\text{data} \\sim \\text{Beta}(\\alpha, \\beta), \\quad \\alpha = k+1, \\; \\beta = n-k+1\n",
    "$\n",
    "\n",
    "**Estimates:**\n",
    "\n",
    "* **MLE:**\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{MLE} = \\frac{k}{n}\n",
    "$\n",
    "\n",
    "* **MAP:**\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{MAP} = \\arg\\max_\\theta p(\\theta \\mid \\text{data})\n",
    "$\n",
    "\n",
    "* **Posterior mean:**\n",
    "\n",
    "$\n",
    "E[\\theta \\mid \\text{data}] = \\frac{k+1}{n+2}\n",
    "$\n",
    "\n",
    "* **Posterior variance:**\n",
    "\n",
    "$\n",
    "\\mathrm{Var}(\\theta \\mid \\text{data}) = \\frac{(k+1)(n-k+1)}{(n+2)^2 (n+3)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e46e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (1)\n",
    "# MAP vs MLE\n",
    "df[\"higher_code\"] = df[\"higher\"].map({\"yes\": 1, \"no\": 0})\n",
    "n = len(df[\"higher_code\"])\n",
    "k = df[\"higher_code\"].sum()\n",
    "\n",
    "theta = np.linspace(0.001, 0.999, 1000)\n",
    "likelihood = theta**k * (1 - theta) ** (n - k)\n",
    "posterior = likelihood / likelihood.sum()\n",
    "\n",
    "theta_map = theta[np.argmax(posterior)]\n",
    "theta_mle = k / n\n",
    "\n",
    "print(\"MLE =\", theta_mle)\n",
    "print(\"MAP =\", theta_map)\n",
    "\n",
    "# Part (2)\n",
    "# Prior and Likelihood\n",
    "prior = np.ones_like(theta)\n",
    "likelihood = theta**k * (1 - theta) ** (n - k)\n",
    "\n",
    "# Part (3)\n",
    "# Bayes numerator and posterior\n",
    "numerator = prior * likelihood\n",
    "posterior = numerator / numerator.sum()\n",
    "\n",
    "# Part (4)\n",
    "# Log-likelihood for stability\n",
    "log_likelihood = k * np.log(theta) + (n - k) * np.log(1 - theta)\n",
    "log_num = log_likelihood\n",
    "posterior_log = np.exp(log_num - log_num.max())\n",
    "posterior_log = posterior_log / posterior_log.sum()\n",
    "\n",
    "# Part (5)\n",
    "# Compare MAP, MLE, and posterior mean\n",
    "theta_map = theta[np.argmax(posterior_log)]\n",
    "theta_mle = k / n\n",
    "posterior_mean = (k + 1) / (n + 2)\n",
    "posterior_variance = ((k + 1) * (n - k + 1)) / ((n + 2)**2 * (n + 3))\n",
    "\n",
    "print(\"MLE =\", theta_mle)\n",
    "print(\"MAP =\", theta_map)\n",
    "print(\"Posterior mean =\", posterior_mean)\n",
    "print(\"Posterior variance =\", posterior_variance)\n",
    "\n",
    "# Part (6)\n",
    "# Theoretical posterior vs Simulation\n",
    "alpha = k + 1\n",
    "beta_param = n - k + 1\n",
    "theory_pdf = beta.pdf(theta, alpha, beta_param)\n",
    "posterior_log_density = posterior_log / (theta[1] - theta[0])\n",
    "\n",
    "plt.plot(theta, posterior_log_density, label=\"Simulation posterior\")\n",
    "plt.plot(theta, theory_pdf, \"--\", label=f\"Theoretical Beta({alpha},{beta_param})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ec4c1",
   "metadata": {},
   "source": [
    "## Question 21\n",
    "\n",
    "---\n",
    "\n",
    "**Hypotheses**\n",
    "\n",
    "* $H_0:$ Rows and columns are independent.\n",
    "* $H_A:$ Rows and columns are dependent.\n",
    "\n",
    "**Expected counts under independence**\n",
    "\n",
    "$\n",
    "E_{ij} = \\frac{(\\text{row total}_i)(\\text{column total}_j)}{n}\n",
    "$\n",
    "\n",
    "**Chi-square statistic**\n",
    "\n",
    "$\n",
    "\\chi^2 = \\sum_{i=1}^{R} \\sum_{j=1}^{C} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n",
    "$\n",
    "\n",
    "**Degrees of freedom**\n",
    "\n",
    "$\n",
    "df = (R - 1)(C - 1)\n",
    "$\n",
    "\n",
    "**Empirical p-value (simulation-based)**\n",
    "\n",
    "$\n",
    "\\hat{p} = \\frac{\\text{\\#}\\{\\chi^2_{\\text{sim}} \\geq \\chi^2_{\\text{obs}}\\}}{\\text{\\# simulations}}\n",
    "$\n",
    "\n",
    "**Theoretical p-value (Chi-square distribution)**\n",
    "\n",
    "$\n",
    "p = 1 - F_{\\chi^2}(\\chi^2_{\\text{obs}}; df)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b978a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (1): Empirical Chi-Squared Distribution\n",
    "# Step 1: Choose dimensions\n",
    "R, C = 3, 4\n",
    "dof = (R - 1) * (C - 1)\n",
    "print(f\"Table dimensions: {R}x{C}, Degrees of freedom = {dof}\")\n",
    "\n",
    "# Step 2: Define null probability model (independence)\n",
    "row_probs = np.array([0.3, 0.5, 0.2])\n",
    "col_probs = np.array([0.2, 0.3, 0.25, 0.25])\n",
    "P = np.outer(row_probs, col_probs)\n",
    "\n",
    "# Step 3: Fix sample size\n",
    "n = 200\n",
    "print(f\"Sample size = {n}\")\n",
    "\n",
    "# Step 4: Simulate many samples under null model\n",
    "num_sims = 10000\n",
    "chi2_values = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for _ in range(num_sims):\n",
    "    sample = np.random.multinomial(n, P.ravel()).reshape(R, C)\n",
    "    O = sample\n",
    "    row_totals = O.sum(axis=1, keepdims=True)\n",
    "    col_totals = O.sum(axis=0, keepdims=True)\n",
    "    E = row_totals @ col_totals / n\n",
    "    chi2_stat = np.sum((O - E) ** 2 / E)\n",
    "    chi2_values.append(chi2_stat)\n",
    "\n",
    "chi2_values = np.array(chi2_values)\n",
    "\n",
    "# Step 5 + 6: Plot histogram with theoretical chi-squared PDF\n",
    "plt.hist(chi2_values, bins=40, density=True, alpha=0.6, label=\"Empirical\")\n",
    "x = np.linspace(0, np.max(chi2_values), 500)\n",
    "plt.plot(x, chi2.pdf(x, dof), \"r-\", lw=2, label=f\"Chi2 PDF (df={dof})\")\n",
    "plt.xlabel(\"Chi-squared statistic\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Empirical vs Theoretical Chi-squared Distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Discussion prints\n",
    "print(\"Part 1 Discussion\")\n",
    "print(\"The empirical histogram closely follows the theoretical Chi-squared distribution.\")\n",
    "print(\"With 10,000 simulations, the match is good. With fewer (e.g., 1,000), variability is higher.\")\n",
    "print(\"If n is too small, expected counts may fall below 5, causing deviations.\")\n",
    "\n",
    "# Part (2): Hypothesis Testing with Alternative Model\n",
    "# Step 1: Define alternative probability model (dependence)\n",
    "P_alt = P.copy()\n",
    "boost = 0.02\n",
    "for i in range(min(R, C)):\n",
    "    P_alt[i, i] += boost\n",
    "# renormalize\n",
    "P_alt = P_alt / P_alt.sum()\n",
    "\n",
    "# Step 2: Draw one sample from alternative model\n",
    "np.random.seed(0)\n",
    "S_alt = np.random.multinomial(n, P_alt.ravel()).reshape(R, C)\n",
    "O_alt = S_alt\n",
    "\n",
    "# Step 3: Compute chi-squared statistic against null expected counts\n",
    "row_totals_alt = O_alt.sum(axis=1, keepdims=True)\n",
    "col_totals_alt = O_alt.sum(axis=0, keepdims=True)\n",
    "E_null = row_totals_alt @ col_totals_alt / n\n",
    "chi2_obs = np.sum((O_alt - E_null) ** 2 / E_null)\n",
    "\n",
    "# Step 4: Compute empirical p-value\n",
    "p_value_emp = np.mean(chi2_values >= chi2_obs)\n",
    "\n",
    "# Step 5: Decision at alpha = 0.1\n",
    "alpha = 0.1\n",
    "if p_value_emp < alpha:\n",
    "    print(\"\\nReject H0: Evidence of dependence between rows and columns.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject H0: No evidence of dependence between rows and columns.\")\n",
    "\n",
    "# Step 6: Compare with theoretical p-value\n",
    "p_value_theory = 1 - chi2.cdf(chi2_obs, dof)\n",
    "\n",
    "print(\"\\nPart 2 Results\")\n",
    "print(\"Observed chi-squared statistic:\", chi2_obs)\n",
    "print(\"Empirical p-value:\", p_value_emp)\n",
    "print(\"Theoretical p-value:\", p_value_theory)\n",
    "\n",
    "print(\"\\nPart 2 Discussion\")\n",
    "print(\"The test rejected the null because dependence was introduced (boosted diagonal).\")\n",
    "print(\"The empirical p-value and theoretical p-value are close, but small differences occur due to simulation variability.\")\n",
    "print(\"The power of the test increases with larger sample size n, making dependence easier to detect.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
